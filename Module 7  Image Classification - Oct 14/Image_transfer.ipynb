{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cd8b78",
   "metadata": {},
   "source": [
    "## Image Classification with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c543a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# deep learning libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import applications\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4baa041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure File Locations\n",
    "\n",
    "# dataset labels\n",
    "labels = pd.read_csv('labels.csv')\n",
    "\n",
    "# folders paths\n",
    "train_path = 'train'\n",
    "test_path = 'test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61a7560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80dd516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add .jpg extension to id to map to file names\n",
    "\n",
    "def to_jpg(id):\n",
    "    return id+\".jpg\"\n",
    "\n",
    "\n",
    "labels['id'] = labels['id'].apply(to_jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4275ee",
   "metadata": {},
   "source": [
    "## The next 3 cells run a data aumentation process to modify the original images for tranfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23031ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data agumentation and pre-processing using tensorflow\n",
    "gen = ImageDataGenerator(\n",
    "                  rescale=1./255.,\n",
    "                  horizontal_flip = True,\n",
    "                  validation_split=0.2 # training: 80% data, validation: 20% data\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c04815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8178 validated image filenames belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = gen.flow_from_dataframe(\n",
    "    labels, # dataframe\n",
    "    directory = train_path, # images data path / folder in which images are there\n",
    "    x_col = 'id',\n",
    "    y_col = 'breed',\n",
    "    subset=\"training\",\n",
    "    color_mode=\"rgb\",\n",
    "    target_size = (331,331), # image height , image width\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8809b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2044 validated image filenames belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = gen.flow_from_dataframe(\n",
    "    labels, # dataframe\n",
    "    directory = train_path, # images data path / folder in which images are there\n",
    "    x_col = 'id',\n",
    "    y_col = 'breed',\n",
    "    subset=\"validation\",\n",
    "    color_mode=\"rgb\",\n",
    "    target_size = (331,331), # image height , image width\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11517e70",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x,y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_generator)\n\u001b[0;32m      2\u001b[0m x\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:112\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m     index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_batches_of_transformed_samples(index_array)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:313\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    311\u001b[0m filepaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepaths\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[1;32m--> 313\u001b[0m     img \u001b[38;5;241m=\u001b[39m image_utils\u001b[38;5;241m.\u001b[39mload_img(\n\u001b[0;32m    314\u001b[0m         filepaths[j],\n\u001b[0;32m    315\u001b[0m         color_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolor_mode,\n\u001b[0;32m    316\u001b[0m         target_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_size,\n\u001b[0;32m    317\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation,\n\u001b[0;32m    318\u001b[0m         keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_aspect_ratio,\n\u001b[0;32m    319\u001b[0m     )\n\u001b[0;32m    320\u001b[0m     x \u001b[38;5;241m=\u001b[39m image_utils\u001b[38;5;241m.\u001b[39mimg_to_array(img, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format)\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# Pillow images should be closed after `load_img`,\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# but not PIL images.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:227\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an image into PIL format.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    A PIL Image instance.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pil_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import PIL.Image. The use of `load_img` requires PIL.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m     )\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, io\u001b[38;5;241m.\u001b[39mBytesIO):\n\u001b[0;32m    231\u001b[0m     img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(path)\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "x,y = next(train_generator)\n",
    "x.shape # input shape of one record is (331,331,3) , 32: is the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574212c",
   "metadata": {},
   "source": [
    "## Display a batch of training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_generator.class_indices\n",
    "class_names = list(a.keys())  # storing class/breed names in a list\n",
    "\n",
    "\n",
    "def plot_images(img, labels):\n",
    "    plt.figure(figsize=[15, 10])\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(img[i])\n",
    "        plt.title(class_names[np.argmax(labels[i])])\n",
    "        plt.axis('off')\n",
    "\n",
    "plot_images(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489b8ca",
   "metadata": {},
   "source": [
    "## Define the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caebeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the InceptionResNetV2 architecture with imagenet weights as base\n",
    "base_model = tf.keras.applications.InceptionResNetV2(\n",
    "                     include_top=False,\n",
    "                     weights='imagenet',\n",
    "                     input_shape=(331,331,3)\n",
    "                     )\n",
    "\n",
    "base_model.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3606914",
   "metadata": {},
   "source": [
    "## Define the transfer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad07a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and pass an input with a defined shape \n",
    "# so the model can infer the shapes of all the layers\n",
    "input_tensor = tf.keras.Input(shape=(331,331,3))\n",
    "output_tensor = base_model(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build the rest of the model\n",
    "model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(120, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce34135",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# categorical cross entropy is taken since its used as a loss function for \n",
    "# multi-class classification problems where there are two or more output labels.\n",
    "# using Adam optimizer for better performance\n",
    "# other optimizers such as sgd can also be used depending upon the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8225750",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e3aac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A callback is an object that can perform actions at various stages of training \n",
    "# (for example, at the start or end of an epoch, before or after a single batch, etc).\n",
    "early = tf.keras.callbacks.EarlyStopping( patience=10,\n",
    "                                          min_delta=0.001,\n",
    "                                          restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400d9ec",
   "metadata": {},
   "source": [
    "## Train the transfer model to tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d06a122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Fit the model by transfering the old weights to the new data scenario\n",
    "# batch_size=32\n",
    "# STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "# STEP_SIZE_VALID = validation_generator.n//validation_generator.batch_size\n",
    "\n",
    "# # fit model\n",
    "# history = model.fit(train_generator,\n",
    "#                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "#                     validation_data=validation_generator,\n",
    "#                     validation_steps=STEP_SIZE_VALID,\n",
    "#                     epochs=25,\n",
    "#                     callbacks=[early])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e4ae4",
   "metadata": {},
   "source": [
    "#### The next 2 cells are used to persist and load the results\n",
    "#### We don't want to lose any progress\n",
    "### Note that the two save files have been loaded to web campus for you to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee011f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Persist the model for later use\n",
    "# # Use Keras for model and pickle for history\n",
    "# import pickle\n",
    "\n",
    "# # save the model\n",
    "# model.save(\"TransferModel_InceptionResNetV2.keras\")\n",
    "\n",
    "# # save the history\n",
    "# with open('history.pkl', 'wb') as file: \n",
    "#     # A new file will be created \n",
    "#     pickle.dump(history, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbb718e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.saving' has no attribute 'load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load Model with Keras\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m persistedModel \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransferModel_InceptionResNetV2.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load Model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# persistedModel = keras.saving.load_model(model_path)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load history with pickle\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file: \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Call load method to deserialze \u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.saving' has no attribute 'load_model'"
     ]
    }
   ],
   "source": [
    "# This is how you load the objects of needed:\n",
    "# Paths to your saved files\n",
    "# model_path = r'H:\\My Drive\\1.a SCHOOL\\1.A) Grad School\\6. Fall 2024\\MIS 776 1001\\Module 7  Image Classification - Oct 14\\TransferModel_InceptionResNetV2.keras'\n",
    "# history_path = r'H:\\My Drive\\1.a SCHOOL\\1.A) Grad School\\6. Fall 2024\\MIS 776 1001\\Module 7  Image Classification - Oct 14\\history.pkl'\n",
    "\n",
    "\n",
    "import pickle\n",
    "# Load Model with Keras\n",
    "persistedModel = keras.saving.load_model(\"TransferModel_InceptionResNetV2.keras\")\n",
    "# Load Model\n",
    "# persistedModel = keras.saving.load_model(model_path)\n",
    "# Load history with pickle\n",
    "with open('history.pkl', 'rb') as file: \n",
    "    # Call load method to deserialze \n",
    "    persistedHistory = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14891a",
   "metadata": {},
   "source": [
    "## Store and display the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a574024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results\n",
    "acc = persistedHistory.history['accuracy']\n",
    "val_acc = persistedHistory.history['val_accuracy']\n",
    "loss = persistedHistory.history['loss']\n",
    "val_loss = persistedHistory.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f9ccdfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5f39f6c8be14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'figure.facecolor'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'white'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Validation Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lower right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "# accuracy\n",
    "plt.figure(figsize=(8, 12))\n",
    "plt.rcParams['figure.figsize'] = [19,9]\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f\"\"\"\\nTraining and Validation Accuracy. \\nTrain Accuracy: \n",
    "          {str(acc[-1])}\\nValidation Accuracy: {str(val_acc[-1])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.title(f\"\"\"Training and Validation Loss. \\nTrain Loss: \n",
    "          {str(loss[-1])}\\nValidation Loss: {str(val_loss[-1])}\"\"\")\n",
    "plt.xlabel('epoch')\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7b844",
   "metadata": {},
   "source": [
    "#### Validate the model using a batch of images from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdbf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score = persistedModel.evaluate(validation_generator)\n",
    "print(accuracy_score)\n",
    "print(\"Accuracy: {:.4f}%\".format(accuracy_score[1] * 100)) \n",
    "\n",
    "print(\"Loss: \",accuracy_score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a564d",
   "metadata": {},
   "source": [
    "## Load and classify a new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = test_path+'/000621fb3cbb32d8935728e48679680e.jpg'\n",
    "\n",
    "img = cv2.imread(test_img_path)\n",
    "resized_img = cv2.resize(img, (331, 331)).reshape(-1, 331, 331, 3)/255\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"TEST IMAGE\")\n",
    "plt.imshow(resized_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "172637f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b3ca7cdf0db3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_resize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m331\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m331\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m331\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m331\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpersistedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img = tf.keras.preprocessing.image.smart_resize(img, (331, 331))\n",
    "img = tf.reshape(img, (-1, 331, 331, 3))\n",
    "prediction = np.argmax(persistedModel.predict(img/255))\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d413224e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: protobuf\n",
      "Version: 3.20.3\n",
      "Summary: Protocol Buffers\n",
      "Home-page: https://developers.google.com/protocol-buffers/\n",
      "Author: \n",
      "Author-email: \n",
      "License: BSD-3-Clause\n",
      "Location: C:\\Users\\zace5\\anaconda3\\envs\\myenv\\Lib\\site-packages\n",
      "Requires: \n",
      "Required-by: tensor, tensorboard, tensorflow_intel\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea4cb09-d1ef-4ecf-a84d-2d9ee28bf00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9d18f8-3251-402b-a5a5-ab6cd4f76c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\zace5\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zace5\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zace5\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zace5\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.5 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 17.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 20.6 MB/s eta 0:00:00\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
